# ETL Climate Insight ğŸŒ

## ğŸ“Œ Deskripsi Proyek

ETL Climate Insight adalah sistem **End-to-End Data Pipeline** untuk memproses data sampah dan limbah dari DKI Jakarta dan KLHK - SIPSN. Seluruh workflow dikelola menggunakan **Apache Airflow** yang berjalan di Docker sehingga user hanya perlu menjalankan **satu perintah**:

```bash
docker compose up -d
```

Pipeline ini berjalan otomatis menggunakan DAG Airflow untuk melakukan proses:

1. **Extract** â€” Membaca data CSV dari folder `raw_data/` (DKI Jakarta & KLHK)
2. **Transform** â€” Membersihkan dan menstrukturkan data menggunakan modul agregasi
3. **Load** â€” Menyimpan hasil ke database SQLite (`climate_data.sqlite`)
4. **Visualisasi** â€” Dashboard Streamlit menampilkan insight data sampah secara interaktif

---

## ğŸ“‚ Struktur Folder

```
ETL-Climate-Insight/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ waste_etl_dag.py      â†’ DAG Airflow untuk ETL sampah
â”‚   â”œâ”€â”€ logs/                     â†’ Log eksekusi Airflow
â”‚   â””â”€â”€ plugins/                  â†’ Airflow plugins (opsional)
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml               â†’ Konfigurasi database & path file
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ etl_pipeline.py           â†’ Script utama ETL
â”‚   â””â”€â”€ agregasi.py               â†’ Modul transformasi data
â”‚
â”œâ”€â”€ db/
â”‚   â””â”€â”€ manager.py                â†’ Database manager (SQLite)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ climate_data.sqlite       â†’ Database SQLite hasil ETL
â”‚   â””â”€â”€ v_jakarta_trend_bulanan.csv â†’ Data agregat Jakarta
â”‚
â”œâ”€â”€ raw_data/
â”‚   â”œâ”€â”€ data_jakarta.csv          â†’ Data mentah DKI Jakarta
â”‚   â””â”€â”€ data_klhk.csv             â†’ Data mentah KLHK - SIPSN
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                    â†’ Aplikasi Streamlit visualisasi
â”‚
â”œâ”€â”€ Dockerfile.airflow            â†’ Dockerfile image Airflow
â”œâ”€â”€ Dockerfile.etl                â†’ Dockerfile environment worker ETL
â”œâ”€â”€ Dockerfile.streamlit          â†’ Dockerfile environment Streamlit
â”‚
â”œâ”€â”€ docker-compose.yaml           â†’ Orkestrasi seluruh service
â”œâ”€â”€ requirements.airflow.txt      â†’ Dependencies Airflow
â”œâ”€â”€ requirements.etl.txt          â†’ Dependencies ETL worker
â”œâ”€â”€ requirements.streamlit.txt    â†’ Dependencies Streamlit
â””â”€â”€ README.md                     â†’ Readme
```

---

## ğŸš€ Arsitektur Sistem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Docker Compose                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚   Airflow    â”‚      â”‚  Streamlit   â”‚                     â”‚
â”‚  â”‚  Webserver   â”‚      â”‚  Dashboard   â”‚                     â”‚
â”‚  â”‚  :8080       â”‚      â”‚  :8501       â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â”‚                     â”‚                             â”‚
â”‚         â”‚                     â”‚ (read)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚   Airflow   â”‚      â”‚  data/         â”‚                    â”‚
â”‚  â”‚  Scheduler  â”œâ”€â”€â”€â”€â”€â–ºâ”‚  climate_data  â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚  .sqlite       â”‚                    â”‚
â”‚         â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â”‚ (trigger)                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚  â”‚  DAG: climate_etl           â”‚                            â”‚
â”‚  â”‚  Task: run_etl_scripts      â”‚                            â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                            â”‚
â”‚  â”‚  â”‚ 1. Extract CSV         â”‚ â”‚                            â”‚
â”‚  â”‚  â”‚ 2. Transform Data      â”‚ â”‚                            â”‚
â”‚  â”‚  â”‚ 3. Load to SQLite      â”‚ â”‚                            â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  raw_data/   â”‚      â”‚  ETL Worker  â”‚                     â”‚
â”‚  â”‚  *.csv       â”‚      â”‚  (standby)   â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Semua komponen berjalan dalam container terpisah yang saling terhubung melalui **Docker Compose**.

---

## âš™ï¸ Komponen Utama

### 1. **Airflow** ğŸ”„

Mengelola dan menjalankan pipeline ETL otomatis:

- **DAG Name**: `climate_etl`
- **Schedule**: Daily (`@daily`)
- **Task**: `run_etl_scripts`
- **Command**: `cd /app && python src/etl_pipeline.py`

Menggunakan **SequentialExecutor** dengan SQLite database agar kompatibel di Windows.

**Web UI**: `http://localhost:8080`
- Username: `admin`
- Password: `admin`

### 2. **ETL Pipeline** ğŸ“Š

Pipeline terdiri dari 3 tahap:

#### Extract
- Membaca `raw_data/data_jakarta.csv` (Data DKI Jakarta)
- Membaca `raw_data/data_klhk.csv` (Data KLHK - SIPSN)

#### Transform
- Menggunakan modul `src/agregasi.py`
- Mendeteksi format tanggal (YYYYMM atau YYYY)
- Membersihkan dan menstrukturkan data
- Menambahkan kolom `sumber_data`

#### Load
- Menyimpan ke SQLite: `data/climate_data.sqlite`
- Tabel: `fact_sampah_harian`
- Total data: ~1787 baris (105 Jakarta + 1682 KLHK)

### 3. **Database SQLite** ğŸ’¾

**File**: `data/climate_data.sqlite`

**Schema Table**: `fact_sampah_harian`
```sql
CREATE TABLE fact_sampah_harian (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    tanggal DATE,
    kecamatan VARCHAR(100),
    volume FLOAT,
    jumlah_trip INT,
    sumber_data VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 4. **Streamlit Dashboard** ğŸ“ˆ

Menampilkan visualisasi data sampah:
- Volume sampah per kecamatan
- Trend bulanan
- Perbandingan antar wilayah
- Grafik interaktif menggunakan Plotly

**Dashboard URL**: `http://localhost:8501`

---

## ğŸ³ Cara Instalasi & Setup

### 1ï¸âƒ£ **Clone / Download Project**

```bash
git clone https://github.com/TaqiyudinMiftah/ETL-Climate-Insight
cd ETL-Climate-Insight
```

### 2ï¸âƒ£ **Pastikan Docker sudah terinstall**

Cek dengan:

```bash
docker --version
docker compose version
```

### 3ï¸âƒ£ **Siapkan Data CSV**

Pastikan file CSV ada di folder `raw_data/`:
- `raw_data/data_jakarta.csv`
- `raw_data/data_klhk.csv`

### 4ï¸âƒ£ **Jalankan Semua Service**

```bash
docker compose up -d
```

Docker akan menjalankan:
- `airflow-init` â†’ Migrate DB + create user admin
- `airflow-scheduler` â†’ Scheduler untuk menjalankan DAG
- `airflow-webserver` â†’ Web UI Airflow
- `etl-worker` â†’ Container standby untuk ETL tasks
- `streamlit` â†’ Dashboard visualisasi

### 5ï¸âƒ£ **Buka Airflow Web UI**

```
http://localhost:8080
```

**Login**:
- Username: `admin`
- Password: `admin`

### 6ï¸âƒ£ **Trigger DAG Pertama Kali**

Di Airflow UI:
1. Cari DAG bernama **`climate_etl`**
2. Klik tombol â–¶ï¸ **Trigger DAG**
3. Tunggu hingga status task menjadi **success** (hijau)

### 7ï¸âƒ£ **Buka Dashboard Streamlit**

```
http://localhost:8501
```

Dashboard akan membaca data dari `data/climate_data.sqlite`.

---

## ğŸ§ª Menjalankan ETL Secara Manual

### Via Airflow UI

1. Buka `http://localhost:8080`
2. Cari DAG **`climate_etl`**
3. Klik tombol â–¶ï¸ **Trigger DAG**
4. Periksa Tree View / Graph View untuk status task

### Via Command Line

Trigger DAG dari terminal:

```bash
docker exec airflow-scheduler airflow dags trigger climate_etl
```

Cek status DAG:

```bash
docker exec airflow-scheduler airflow dags list
```

Lihat log task:

```bash
docker logs airflow-scheduler --tail 100
```

### Verifikasi Data di Database

```bash
# Cek file database
docker exec airflow-scheduler ls -lh /app/data/

# Query jumlah data
docker exec airflow-scheduler sqlite3 /app/data/climate_data.sqlite "SELECT COUNT(*) FROM fact_sampah_harian;"

# Query per sumber data
docker exec airflow-scheduler sqlite3 /app/data/climate_data.sqlite "SELECT sumber_data, COUNT(*) FROM fact_sampah_harian GROUP BY sumber_data;"
```

**Output yang diharapkan**:
- DKI Jakarta: 105 baris
- KLHK - SIPSN: 1682 baris
- **Total**: 1787 baris

---

---

## ğŸ’¡ Teknologi yang Digunakan

| Teknologi | Versi | Fungsi |
|-----------|-------|--------|
| **Apache Airflow** | 2.9.2 | Orchestration & Scheduling |
| **Python** | 3.10 | Programming Language |
| **SQLite** | Latest | Database Storage |
| **Streamlit** | Latest | Dashboard & Visualization |
| **Docker** | Latest | Containerization |
| **Docker Compose** | Latest | Multi-container Orchestration |
| **Pandas** | Latest | Data Manipulation |
| **SQLAlchemy** | Latest | Database ORM |
| **PyYAML** | Latest | Configuration Management |

---

## âœ¨ Fitur Utama

âœ… **Automated ETL Pipeline** - Berjalan otomatis setiap hari via Airflow Scheduler

âœ… **Multi-Source Data** - Menggabungkan data dari DKI Jakarta & KLHK - SIPSN

âœ… **Data Validation** - Auto-detect format tanggal (YYYYMM / YYYY)

âœ… **SQLite Database** - Lightweight, no external database server needed

âœ… **Docker-based** - Portable, consistent environment across platforms

âœ… **Real-time Monitoring** - Airflow Web UI untuk monitoring pipeline

âœ… **Interactive Dashboard** - Streamlit dashboard dengan visualisasi Plotly

âœ… **Scalable Architecture** - Mudah ditambah sumber data baru

---

## ğŸ“Š Data Source

### 1. DKI Jakarta (`data_jakarta.csv`)
- Format: YYYYMM (Bulanan)
- Cakupan: Data sampah per kecamatan di Jakarta
- Jumlah: ~105 records

### 2. KLHK - SIPSN (`data_klhk.csv`)
- Format: YYYY (Tahunan)
- Cakupan: Data sampah nasional
- Jumlah: ~1682 records

---

## ğŸ›  Troubleshooting & Tips

### â— DAG tidak muncul di Airflow UI

**Solusi:**
```bash
# Restart scheduler
docker compose restart airflow-scheduler

# Cek logs
docker logs airflow-scheduler --tail 50
```

### â— Task gagal dengan error "Can not find the cwd: /app"

**Penyebab**: Konfigurasi DAG salah

**Solusi**: Pastikan di `waste_etl_dag.py`:
```python
run_etl = BashOperator(
    task_id="run_etl_scripts",
    bash_command="cd /app && python src/etl_pipeline.py"
)
```

### â— Database kosong setelah ETL

**Cek logs task**:
```bash
docker exec airflow-scheduler cat /opt/airflow/logs/dag_id=climate_etl/run_id=manual__*/task_id=run_etl_scripts/attempt=1.log
```

**Pastikan file CSV ada**:
```bash
docker exec airflow-scheduler ls -lh /app/raw_data/
```

### â— Error "ModuleNotFoundError"

**Solusi**: Rebuild containers
```bash
docker compose down
docker compose build --no-cache
docker compose up -d
```

### â— Port 8080 atau 8501 sudah digunakan

**Solusi**: Edit `docker-compose.yaml`
```yaml
# Ubah port mapping
ports:
  - "8081:8080"  # Airflow
  - "8502:8501"  # Streamlit
```

### ğŸ”„ Reset Semua Data

```bash
# Stop dan hapus semua container + volume
docker compose down -v

# Rebuild dan start ulang
docker compose up -d
```

### ğŸ“‹ Melihat Resource Usage

```bash
# CPU dan Memory usage
docker stats

# Disk usage
docker system df
```

---

## ğŸš€ Pengembangan Selanjutnya

Beberapa ide untuk pengembangan:

- [ ] Migrasi ke PostgreSQL untuk production
- [ ] Tambahkan data source API real-time
- [ ] Implementasi data quality checks
- [ ] Alert notification (email/Slack) jika pipeline gagal
- [ ] Dashboard analytics lebih advanced
- [ ] Export data ke format lain (CSV, Excel, JSON)
- [ ] Implementasi data versioning
- [ ] Auto-backup database

---

## ğŸ“ Catatan Penting

âš ï¸ **Database SQLite** cocok untuk development/testing. Untuk production dengan volume data besar, disarankan migrasi ke PostgreSQL/MySQL.

âš ï¸ **SequentialExecutor** hanya bisa menjalankan 1 task pada satu waktu. Untuk parallel execution, gunakan LocalExecutor atau CeleryExecutor.

âš ï¸ Pastikan folder `raw_data/` berisi file CSV sebelum menjalankan DAG pertama kali.

---

## ğŸ“„ Lisensi

Project ini dibuat untuk keperluan pembelajaran dan portfolio

---

## ğŸ‘¤ Author

**Taqiyudin Miftah**
- GitHub: [@TaqiyudinMiftah](https://github.com/TaqiyudinMiftah)
- GitHub: [@Flax9](https://github.com/Flax9)

---

## ğŸ™ Acknowledgments

- Apache Airflow Documentation
- Streamlit Community
- Docker Documentation
- Python Community

---

**â­ Jika project ini bermanfaat, jangan lupa untuk memberikan star di GitHub!**
