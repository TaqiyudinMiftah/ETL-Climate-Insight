# ğŸŒ ETL Climate Insight â€” Unified Dockerized Data Engineering Pipeline

ETL Climate Insight adalah project **end-to-end Data Engineering** yang memproses data sampah Jakarta & KLHK melalui pipeline **ETL (Extract â†’ Transform â†’ Load)**, menjalankan orchestrasi otomatis dengan **Apache Airflow**, serta menampilkan analisanya melalui **Streamlit Dashboard**.

Kini project ini telah di-*refactor* agar **cukup dijalankan dengan satu perintah:**

```bash
docker compose up -d
```

Dan seluruh layanan berikut akan otomatis menyala:

* ğŸŒ€ Airflow Webserver
* ğŸŒ€ Airflow Scheduler
* ğŸ—ƒï¸ PostgreSQL untuk metadata Airflow
* ğŸ“Š Streamlit Dashboard (automatically runs)
* âš™ï¸ ETL Runner (opsional menjalankan pipeline Python di startup)

---

# ğŸ“ 1. Struktur Project

```
ETL-Climate-Insight/
â”‚â”€â”€ docker-compose.yaml
â”‚â”€â”€ Dockerfile.airflow
â”‚â”€â”€ Dockerfile.streamlit
â”‚â”€â”€ Dockerfile.etl
â”‚â”€â”€ .env
â”‚â”€â”€ airflow/
â”‚     â”œâ”€â”€ dags/
â”‚     â”œâ”€â”€ logs/
â”‚     â””â”€â”€ plugins/
â”‚â”€â”€ src/
â”‚â”€â”€ dashboard/
â”‚â”€â”€ config/
â”‚â”€â”€ data/
â”‚â”€â”€ raw_data/
â”‚â”€â”€ setup_sqlite.py
â”‚â”€â”€ requirements.txt
```

---

# ğŸ”§ 2. Setup Environment

## 2.1 Install dependencies (opsional jika pakai Docker)

```bash
pip install -r requirements.txt
```

## 2.2 Buat file `.env`

Buat file di root project:

```
.env
```

Isi:

```env
AIRFLOW_UID=50000
FERNET_KEY=<masukkan-fernet-key-valid>
```

Generate key valid:

```bash
python - <<EOF
from cryptography.fernet import Fernet
print(Fernet.generate_key().decode())
EOF
```

---

# ğŸ“¦ 3. Menjalankan Semua Layanan (Airflow + Streamlit + ETL)

Jalankan dari root folder:

```bash
docker compose up -d
```

Setelah itu:

* Airflow Web UI â†’ [http://localhost:8081](http://localhost:8081)
* Streamlit Dashboard â†’ [http://localhost:8501](http://localhost:8501)

Jika pertama kali (Airflow DB belum di-init), jalankan:

```bash
docker compose run airflow-webserver airflow db init
```

Lalu buat user admin:

```bash
docker compose run airflow-webserver airflow users create \
  --username admin \
  --password admin \
  --firstname Air \
  --lastname Flow \
  --role Admin \
  --email admin@example.com
```

Kemudian restart:

```bash
docker compose up -d
```

---

# ğŸ”„ 4. ETL Pipeline

Pipeline terdiri dari:

1. **Extract** â€” membaca data dari `raw_data/`
2. **Transform** â€” normalisasi, pembersihan, agregasi tren
3. **Load** â€” menyimpan hasil ke SQLite (untuk dashboard)

Pipeline manual:

```bash
python -m src.etl_pipeline
python setup_sqlite.py
```

---

# ğŸ§  5. Airflow DAG

DAG otomatis:

```
check_raw_data_files
    â†’ run_etl_pipeline
        â†’ build_sqlite_for_dashboard
            â†’ pipeline_completed
```

DAG terletak di:

```
airflow/dags/waste_etl_dag.py
```

---

# ğŸ“Š 6. Streamlit Dashboard

Dashboard memuat:

* Tren total volume sampah per bulan
* Tren per kecamatan
* Heatmap waktuâ€“wilayah
* Insight otomatis (anomali)

Menjalankan dashboard manual:

```bash
streamlit run dashboard/app.py
```

---

# âš™ï¸ 7. Konfigurasi (config.yaml)

```yaml
paths:
  raw_data_dir: raw_data
  data_dir: data
  output_sqlite: data/v_jakarta_trend_bulanan.sqlite

database:
  sqlite:
    file_name: v_jakarta_trend_bulanan.sqlite
    table_name: v_jakarta_trend_bulanan
```

---

# ğŸš€ 8. Rencana Pengembangan

* Integrasi API real-time
* Data Quality Check otomatis
* Notifikasi Airflow â†’ Telegram
* Streaming pipeline (Kafka/Spark)
* Deployment dashboard ke Streamlit Cloud
